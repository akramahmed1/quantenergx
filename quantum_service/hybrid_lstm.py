"""
Quantum-Enhanced LSTM for Energy Price Forecasting
Combines classical LSTM with quantum variational circuits for improved prediction accuracy
"""

import os
import logging
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime, timedelta
import json

# Quantum imports with fallback handling
try:
    from qiskit import QuantumCircuit, transpile
    from qiskit.circuit import ParameterVector
    from qiskit.primitives import Estimator
    from qiskit_ibm_runtime import QiskitRuntimeService, Session, Estimator as RuntimeEstimator
    from qiskit.quantum_info import SparsePauliOp
    QUANTUM_AVAILABLE = True
except ImportError:
    QUANTUM_AVAILABLE = False
    logging.warning("Qiskit not available. Running in classical mode.")

from flask import Flask, request, jsonify
from flask_cors import CORS

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class QuantumVariationalLayer(nn.Module):
    """
    Quantum Variational Layer that can fallback to classical computation
    """
    
    def __init__(self, input_size: int, quantum_enabled: bool = True):
        super().__init__()
        self.input_size = input_size
        self.quantum_enabled = quantum_enabled and QUANTUM_AVAILABLE
        self.n_qubits = min(4, input_size)  # Limit to 4 qubits for NISQ devices
        
        if self.quantum_enabled:
            self._init_quantum_circuit()
        else:
            # Classical fallback: simple linear transformation
            self.classical_layer = nn.Linear(input_size, input_size)
            
    def _init_quantum_circuit(self):
        """Initialize quantum variational circuit"""
        self.circuit = QuantumCircuit(self.n_qubits)
        self.params = ParameterVector('Î¸', self.n_qubits * 2)
        
        # Parameterized quantum circuit with RY and RZ gates
        for i in range(self.n_qubits):
            self.circuit.ry(self.params[i], i)
            self.circuit.rz(self.params[i + self.n_qubits], i)
            
        # Entangling gates
        for i in range(self.n_qubits - 1):
            self.circuit.cx(i, i + 1)
            
        # Measurement observables (Pauli-Z on each qubit)
        self.observables = [SparsePauliOp(f"{'I' * i}Z{'I' * (self.n_qubits - i - 1)}") 
                           for i in range(self.n_qubits)]
        
        self.estimator = self._get_estimator()
        
    def _get_estimator(self):
        """Get quantum estimator with IBM hardware fallback"""
        try:
            # Try to connect to IBM Quantum
            if os.getenv('IBMQ_TOKEN'):
                service = QiskitRuntimeService(
                    channel="ibm_quantum",
                    token=os.getenv('IBMQ_TOKEN')
                )
                backend = service.least_busy(operational=True, simulator=False)
                logger.info(f"Using IBM Quantum backend: {backend.name}")
                return RuntimeEstimator(backend=backend)
            else:
                logger.info("IBMQ_TOKEN not found, using local simulator")
                return Estimator()
        except Exception as e:
            logger.warning(f"Failed to connect to IBM Quantum: {e}. Using local simulator.")
            return Estimator()
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass through quantum variational layer"""
        if not self.quantum_enabled:
            return torch.tanh(self.classical_layer(x))
            
        batch_size = x.shape[0]
        output = torch.zeros_like(x[:, :self.n_qubits])
        
        for i in range(batch_size):
            # Convert input to quantum parameters
            params_values = x[i, :self.n_qubits * 2].detach().numpy()
            
            try:
                # Execute quantum circuit
                job = self.estimator.run(
                    [self.circuit] * self.n_qubits,
                    self.observables,
                    [params_values] * self.n_qubits
                )
                result = job.result()
                
                # Extract expectation values
                expectations = [result.values[j] for j in range(self.n_qubits)]
                output[i] = torch.tensor(expectations, dtype=torch.float32)
                
            except Exception as e:
                logger.warning(f"Quantum execution failed: {e}. Using classical fallback.")
                # Classical fallback for this sample
                output[i] = torch.tanh(x[i, :self.n_qubits])
                
        return output

class HybridLSTM(nn.Module):
    """
    Hybrid Quantum-Classical LSTM for energy price forecasting
    """
    
    def __init__(self, input_size: int, hidden_size: int, num_layers: int = 2, 
                 quantum_enabled: bool = True, dropout: float = 0.2):
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.quantum_enabled = quantum_enabled
        
        # Quantum preprocessing layer
        self.quantum_layer = QuantumVariationalLayer(input_size, quantum_enabled)
        
        # Classical LSTM layers
        self.lstm = nn.LSTM(
            input_size, hidden_size, num_layers, 
            batch_first=True, dropout=dropout
        )
        
        # Output projection
        self.fc = nn.Linear(hidden_size, 1)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass through hybrid model"""
        batch_size, seq_len, features = x.shape
        
        # Apply quantum layer to each timestep
        quantum_enhanced = torch.zeros_like(x)
        for t in range(seq_len):
            if self.quantum_enabled and features >= 4:
                quantum_enhanced[:, t, :] = self.quantum_layer(x[:, t, :])
            else:
                quantum_enhanced[:, t, :] = x[:, t, :]
                
        # LSTM forward pass
        lstm_out, _ = self.lstm(quantum_enhanced)
        
        # Use only the last timestep
        output = self.fc(self.dropout(lstm_out[:, -1, :]))
        return output

class QuantumLSTMForecaster:
    """
    Main forecasting service with quantum-enhanced LSTM
    """
    
    def __init__(self, quantum_enabled: bool = True):
        self.quantum_enabled = quantum_enabled and QUANTUM_AVAILABLE
        self.model = None
        self.scaler = None
        self.is_trained = False
        
        # Model hyperparameters
        self.sequence_length = 24  # 24 hours of historical data
        self.features = ['price', 'volume', 'volatility', 'demand']
        
        logger.info(f"QuantumLSTMForecaster initialized (quantum_enabled={self.quantum_enabled})")
        
    def prepare_data(self, data: pd.DataFrame) -> Tuple[torch.Tensor, torch.Tensor]:
        """Prepare and scale time series data"""
        from sklearn.preprocessing import StandardScaler
        
        # Select features
        feature_cols = [col for col in self.features if col in data.columns]
        if not feature_cols:
            raise ValueError(f"None of the required features {self.features} found in data")
            
        features = data[feature_cols].values
        
        # Scale features
        if self.scaler is None:
            self.scaler = StandardScaler()
            scaled_features = self.scaler.fit_transform(features)
        else:
            scaled_features = self.scaler.transform(features)
            
        # Create sequences
        X, y = [], []
        for i in range(len(scaled_features) - self.sequence_length):
            X.append(scaled_features[i:i + self.sequence_length])
            y.append(scaled_features[i + self.sequence_length, 0])  # Predict price
            
        return torch.FloatTensor(X), torch.FloatTensor(y)
    
    def train(self, training_data: pd.DataFrame, epochs: int = 50) -> Dict[str, Any]:
        """Train the hybrid quantum-classical model"""
        logger.info("Starting model training...")
        
        # Prepare data
        X, y = self.prepare_data(training_data)
        
        # Initialize model
        input_size = X.shape[2]
        hidden_size = 64
        self.model = HybridLSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            quantum_enabled=self.quantum_enabled
        )
        
        # Training setup
        criterion = nn.MSELoss()
        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)
        
        # Split data
        train_size = int(0.8 * len(X))
        X_train, X_val = X[:train_size], X[train_size:]
        y_train, y_val = y[:train_size], y[train_size:]
        
        # Training loop
        train_losses = []
        val_losses = []
        
        for epoch in range(epochs):
            # Training
            self.model.train()
            optimizer.zero_grad()
            
            train_pred = self.model(X_train)
            train_loss = criterion(train_pred.squeeze(), y_train)
            train_loss.backward()
            optimizer.step()
            
            # Validation
            self.model.eval()
            with torch.no_grad():
                val_pred = self.model(X_val)
                val_loss = criterion(val_pred.squeeze(), y_val)
                
            train_losses.append(train_loss.item())
            val_losses.append(val_loss.item())
            
            if epoch % 10 == 0:
                logger.info(f"Epoch {epoch}: Train Loss = {train_loss.item():.6f}, "
                           f"Val Loss = {val_loss.item():.6f}")
        
        self.is_trained = True
        logger.info("Training completed successfully")
        
        return {
            'train_loss': train_losses[-1],
            'val_loss': val_losses[-1],
            'quantum_enabled': self.quantum_enabled,
            'epochs': epochs
        }
    
    def predict(self, recent_data: pd.DataFrame, hours_ahead: int = 24) -> Dict[str, Any]:
        """Generate price forecasts"""
        if not self.is_trained:
            raise ValueError("Model must be trained before making predictions")
            
        logger.info(f"Generating forecast for {hours_ahead} hours ahead...")
        
        # Prepare input data
        X, _ = self.prepare_data(recent_data)
        
        # Use the last sequence for prediction
        last_sequence = X[-1:] if len(X) > 0 else torch.zeros(1, self.sequence_length, len(self.features))
        
        self.model.eval()
        predictions = []
        
        with torch.no_grad():
            current_sequence = last_sequence.clone()
            
            for _ in range(hours_ahead):
                # Predict next value
                pred = self.model(current_sequence)
                predictions.append(pred.item())
                
                # Update sequence (simple approach - in practice, you'd include other features)
                new_row = torch.zeros(1, 1, current_sequence.shape[2])
                new_row[0, 0, 0] = pred  # Update price prediction
                
                # Shift sequence and add new prediction
                current_sequence = torch.cat([
                    current_sequence[:, 1:, :],
                    new_row
                ], dim=1)
        
        # Inverse transform predictions (approximate)
        if self.scaler is not None:
            # Create dummy array for inverse transform
            dummy = np.zeros((len(predictions), len(self.features)))
            dummy[:, 0] = predictions
            predictions_rescaled = self.scaler.inverse_transform(dummy)[:, 0]
        else:
            predictions_rescaled = predictions
            
        return {
            'predictions': predictions_rescaled.tolist(),
            'hours_ahead': hours_ahead,
            'model_type': 'quantum_lstm' if self.quantum_enabled else 'classical_lstm',
            'timestamp': datetime.now().isoformat(),
            'confidence_interval': self._calculate_confidence_interval(predictions_rescaled)
        }
    
    def _calculate_confidence_interval(self, predictions: np.ndarray, confidence: float = 0.95) -> Dict[str, List[float]]:
        """Calculate confidence intervals for predictions"""
        # Simple approach using standard deviation
        std = np.std(predictions)
        z_score = 1.96  # 95% confidence
        
        lower_bound = predictions - z_score * std
        upper_bound = predictions + z_score * std
        
        return {
            'lower': lower_bound.tolist(),
            'upper': upper_bound.tolist(),
            'confidence': confidence
        }
    
    def benchmark_vs_classical(self, test_data: pd.DataFrame) -> Dict[str, Any]:
        """Compare quantum vs classical LSTM performance"""
        logger.info("Running quantum vs classical benchmark...")
        
        # Prepare test data
        X_test, y_test = self.prepare_data(test_data)
        
        results = {}
        
        for model_type in ['classical', 'quantum']:
            # Create model
            input_size = X_test.shape[2]
            model = HybridLSTM(
                input_size=input_size,
                hidden_size=64,
                quantum_enabled=(model_type == 'quantum')
            )
            
            # Quick training (simplified)
            criterion = nn.MSELoss()
            optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
            
            model.train()
            for _ in range(20):  # Quick training
                optimizer.zero_grad()
                pred = model(X_test[:50])  # Use subset for speed
                loss = criterion(pred.squeeze(), y_test[:50])
                loss.backward()
                optimizer.step()
            
            # Evaluate
            model.eval()
            with torch.no_grad():
                predictions = model(X_test)
                mse = criterion(predictions.squeeze(), y_test).item()
                mae = torch.mean(torch.abs(predictions.squeeze() - y_test)).item()
                
            results[model_type] = {
                'mse': mse,
                'mae': mae,
                'rmse': np.sqrt(mse)
            }
        
        # Calculate improvement
        improvement_mse = (results['classical']['mse'] - results['quantum']['mse']) / results['classical']['mse'] * 100
        
        return {
            'classical_lstm': results['classical'],
            'quantum_lstm': results['quantum'],
            'improvement_percentage': improvement_mse,
            'quantum_advantage': improvement_mse > 0
        }

# Flask web service
app = Flask(__name__)
CORS(app)

# Global forecaster instance
forecaster = QuantumLSTMForecaster()

@app.route('/health', methods=['GET'])
def health_check():
    """Health check endpoint"""
    return jsonify({
        'status': 'healthy',
        'quantum_available': QUANTUM_AVAILABLE,
        'service': 'quantum_lstm_forecaster',
        'timestamp': datetime.now().isoformat()
    })

@app.route('/forecast', methods=['POST'])
def generate_forecast():
    """Generate energy price forecast"""
    try:
        data = request.get_json()
        
        # Validate input
        if 'historical_data' not in data:
            return jsonify({'error': 'historical_data required'}), 400
            
        # Convert to DataFrame
        df = pd.DataFrame(data['historical_data'])
        hours_ahead = data.get('hours_ahead', 24)
        
        # Train if not already trained (in practice, you'd use pre-trained models)
        if not forecaster.is_trained:
            training_result = forecaster.train(df)
            logger.info(f"Model trained with result: {training_result}")
        
        # Generate forecast
        forecast = forecaster.predict(df, hours_ahead)
        
        return jsonify({
            'success': True,
            'forecast': forecast
        })
        
    except Exception as e:
        logger.exception("Forecast generation failed")
        return jsonify({'error': 'An internal error has occurred.'}), 500

@app.route('/benchmark', methods=['POST'])
def run_benchmark():
    """Run quantum vs classical benchmark"""
    try:
        data = request.get_json()
        
        if 'test_data' not in data:
            return jsonify({'error': 'test_data required'}), 400
            
        df = pd.DataFrame(data['test_data'])
        results = forecaster.benchmark_vs_classical(df)
        
        return jsonify({
            'success': True,
            'benchmark_results': results
        })
        
    except Exception as e:
        logger.exception("Benchmark failed")
        return jsonify({'error': 'An internal error has occurred.'}), 500

if __name__ == '__main__':
    port = int(os.getenv('PORT', 5000))
    debug = os.getenv('DEBUG', 'False').lower() == 'true'
    
    logger.info(f"Starting Quantum LSTM Forecaster on port {port}")
    logger.info(f"Quantum computing available: {QUANTUM_AVAILABLE}")
    
    app.run(host='0.0.0.0', port=port, debug=debug)